{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from Dataset import SpeechDataset\n",
    "from DataLoader import DataLoader\n",
    "from Speech import Speech\n",
    "from preprocessors import *\n",
    "from utils import scrape_speeches\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "dataset_types = [\"important\", \"typical\"]\n",
    "resources_dir = f\"{cwd}/resources\"\n",
    "saving_dir = f\"{cwd}/resources/dataset_all.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(saving_dir):\n",
    "    df_dict = {\n",
    "        \"speaker\": [], \"title\": [], \"year\": [], \"content\": [],\n",
    "        \"anger\": [], \"disgust\": [], \"disgust\": [], \"joy\": [], \"neutral\": [], \"sadness\": [], \"surprise\": [],\n",
    "        \"polarity\": [], \"subjectivity\": [], \"complexity\": [], \"lexical_richness\": [],\n",
    "        \"entities_proportion_in_speech\": [], \"imagery_proportion_in_speech\": [],\n",
    "        \"stopwords_proportion_in_speech\": [], \"mean_sentence_length\": [],\n",
    "        \"label\": []\n",
    "    }\n",
    "\n",
    "    imagery_words = pd.read_csv(\"resources/visual_words.csv\", header=None)\n",
    "    imagery_words = list(imagery_words[0].array)\n",
    "    stop_words = list(spacy.load(\"en_core_web_md\").Defaults.stop_words)\n",
    "\n",
    "    for dataset_type in dataset_types:\n",
    "        path = f\"{cwd}/dataset/{dataset_type}\"\n",
    "        dataset = SpeechDataset(path)\n",
    "        dataloader = DataLoader(dataset)\n",
    "        with tqdm(total=len(dataloader.dataset)) as progress_bar:\n",
    "            for speech in dataloader:\n",
    "                for key in df_dict.keys():\n",
    "                    try:\n",
    "                        df_dict[key].append(getattr(speech, f\"get_{key}\")())\n",
    "                    except:\n",
    "                        pass\n",
    "                emotions = speech.get_emotion_scores(return_all_scores=True)[0]\n",
    "                for emotion in emotions:\n",
    "                    df_dict[emotion[\"label\"]].append(emotion[\"score\"])\n",
    "\n",
    "                df_dict[\"entities_proportion_in_speech\"].append(speech.get_proportion_in_speech(speech.get_entities()))\n",
    "                df_dict[\"imagery_proportion_in_speech\"].append(speech.get_proportion_in_speech(imagery_words))\n",
    "                df_dict[\"stopwords_proportion_in_speech\"].append(speech.get_proportion_in_speech(stop_words))\n",
    "                if dataset_type == \"important\":\n",
    "                    df_dict[\"label\"].append(1.0)\n",
    "                else:\n",
    "                    df_dict[\"label\"].append(0.0)\n",
    "                progress_bar.update(1)\n",
    "\n",
    "    if not os.path.exists(resources_dir):\n",
    "        os.mkdir(resources_dir)\n",
    "    df = pd.DataFrame(df_dict)\n",
    "    df.to_csv(saving_dir)\n",
    "else:\n",
    "    df = pd.read_csv(saving_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### EDA\n",
    "\n",
    "##### More speeches with high fear (> 0.2) in important than typical.\n",
    "##### Fewer speeches with high joy (> 0.2) in important than typical.\n",
    "##### Fewer speeches with high neutrality (0.2) in important than typical.\n",
    "##### Fewer speeches with low surprise (< 0.2) in important than typical.\n",
    "##### Fewer speeches with polarity (>0.15) in important than typical.\n",
    "##### More speeches with high subjectivity (>0.4) in important than typical.\n",
    "\n",
    "##### Fewer speeches with high proportion of entities (> 0.02) in imporant than typical.\n",
    "##### Fewer speeches with high proportion of imagery words (>0.075) in important than in typical.\n",
    "##### Fewer speaches with high proportion (>0.05) of stopwords in important than in typical.\n",
    "##### Fewer speeches with high complexity (>60) in important than in typical.\n",
    "##### Fewer speeches with high lexical richness (>0.3) in important than in typical.\n",
    "##### More speeches with high (>20) mean sentence length in important than in typical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for colname in df.columns[5:-1]:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20, 10))\n",
    "    ax1.bar(list(range(0, len(df[df[\"label\"]==1][colname].array))), df[df[\"label\"]==1][colname].array)\n",
    "    ax1.set_xlabel(\"Speech Name\")\n",
    "    ax1.set_ylabel(f\"{colname} score\")\n",
    "    ax1.set_title(f\"Important Speeches vs {colname} Score\")\n",
    "    ax2.bar(list(range(0, len(df[df[\"label\"]==0][colname].array))), df[df[\"label\"]==0][colname].array)\n",
    "    ax2.set_xlabel(\"Speech Name\")\n",
    "    ax2.set_ylabel(f\"{colname} score\")\n",
    "    ax2.set_title(f\"Typical Speeches vs {colname} Score\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Classification using SVM with RBF Kernel\n",
    "\n",
    "##### Accuracy on test: 78.72%\n",
    "##### Precision on test: 0.74\n",
    "##### Sensitivity on test: 0.86\n",
    "##### AUC on test: 0.78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = np.array(df.iloc[:, 5:-1])\n",
    "target = np.array(df.iloc[:, -1])\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, \n",
    "                                                    target, \n",
    "                                                    test_size=0.3, \n",
    "                                                    stratify=target, random_state=109)\n",
    "param_grid = {'svm__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "              'svm__gamma': [100, 10, 1, 0.1, 0.2, 0.02, 0.002, 0.0002, 0.01, 0.001, 0.0001],\n",
    "              'svm__kernel': ['rbf']}\n",
    "pipeline = Pipeline(steps = [(\"StandardScaler\", StandardScaler()), (\"svm\", svm.SVC())])\n",
    "search = GridSearchCV(pipeline, param_grid, scoring=\"accuracy\", cv=5, refit = True, verbose = 0, n_jobs=5)\n",
    "search.fit(X_train, y_train)\n",
    "y_pred = search.predict(X_test)\n",
    "\n",
    "print(\"Mean Accuracy on test:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Mean Precision on test:\", metrics.precision_score(y_test, y_pred))\n",
    "print(\"Mean Sensitivity on test:\", metrics.recall_score(y_test, y_pred))\n",
    "print(\"Mean AUC over on test:\", metrics.roc_auc_score(y_test, y_pred))\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(cm).plot()\n",
    "\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, y_pred, pos_label=search.classes_[1])\n",
    "roc_display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr).plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mda-project",
   "language": "python",
   "name": "mda-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}