{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from preprocessors import *\n",
    "from sklearn.metrics import *\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from Dataset import SpeechDataset\n",
    "from DataLoader import DataLoader\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import *\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "dataset_types = [\"important\", \"typical\"]\n",
    "resources_dir = f\"{cwd}/resources\"\n",
    "saving_dir = f\"{cwd}/resources/dataset_all.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Dataset Creation and Extraction of Importance Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(saving_dir):\n",
    "    df_dict = {\n",
    "        \"speaker\": [], \"title\": [], \"year\": [], \"content\": [],\n",
    "        \"anger\": [], \"disgust\": [], \"disgust\": [], \"joy\": [], \"neutral\": [], \"sadness\": [], \"surprise\": [],\n",
    "        \"polarity\": [], \"subjectivity\": [], \"complexity\": [], \"lexical_richness\": [],\n",
    "        \"entities_proportion_in_speech\": [], \"imagery_proportion_in_speech\": [],\n",
    "        \"stopwords_proportion_in_speech\": [], \"mean_sentence_length\": [],\n",
    "        \"label\": []\n",
    "    }\n",
    "\n",
    "    imagery_words = pd.read_csv(\"resources/visual_words.csv\", header=None)\n",
    "    imagery_words = list(imagery_words[0].array)\n",
    "    stop_words = list(spacy.load(\"en_core_web_md\").Defaults.stop_words)\n",
    "\n",
    "    for dataset_type in dataset_types:\n",
    "        path = f\"{cwd}/dataset/{dataset_type}\"\n",
    "        dataset = SpeechDataset(path)\n",
    "        dataloader = DataLoader(dataset)\n",
    "        with tqdm(total=len(dataloader.dataset)) as progress_bar:\n",
    "            for speech in dataloader:\n",
    "                for key in df_dict.keys():\n",
    "                    try:\n",
    "                        df_dict[key].append(getattr(speech, f\"get_{key}\")())\n",
    "                    except:\n",
    "                        pass\n",
    "                emotions = speech.get_emotion_scores(return_all_scores=True)[0]\n",
    "                for emotion in emotions:\n",
    "                    df_dict[emotion[\"label\"]].append(emotion[\"score\"])\n",
    "\n",
    "                df_dict[\"entities_proportion_in_speech\"].append(speech.get_proportion_in_speech(speech.get_entities()))\n",
    "                df_dict[\"imagery_proportion_in_speech\"].append(speech.get_proportion_in_speech(imagery_words))\n",
    "                df_dict[\"stopwords_proportion_in_speech\"].append(speech.get_proportion_in_speech(stop_words))\n",
    "                if dataset_type == \"important\":\n",
    "                    df_dict[\"label\"].append(1.0)\n",
    "                else:\n",
    "                    df_dict[\"label\"].append(0.0)\n",
    "                progress_bar.update(1)\n",
    "\n",
    "    if not os.path.exists(resources_dir):\n",
    "        os.mkdir(resources_dir)\n",
    "    df = pd.DataFrame(df_dict)\n",
    "    df.to_csv(saving_dir)\n",
    "else:\n",
    "    df = pd.read_csv(saving_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Importance Measures and EDA\n",
    "\n",
    "##### More speeches with high fear (> 0.2) in important than typical.\n",
    "##### Fewer speeches with high joy (> 0.2) in important than typical.\n",
    "##### Fewer speeches with high neutrality (> 0.2) in important than typical.\n",
    "##### Fewer speeches with low surprise (< 0.2) in important than typical.\n",
    "##### Fewer speeches with polarity (>0.15) in important than typical.\n",
    "##### More speeches with high subjectivity (>0.4) in important than typical.\n",
    "##### Fewer speeches with low complexity (>60) in important than in typical.\n",
    "##### Fewer speeches with high lexical richness (>0.3) in important than in typical.\n",
    "##### Fewer speeches with high proportion of entities (> 0.02) in imporant than typical.\n",
    "##### Fewer speeches with high proportion of imagery words (>0.075) in important than in typical.\n",
    "##### Fewer speaches with high proportion (>0.05) of stopwords in important than in typical.\n",
    "##### More speeches with high (>20) mean sentence length in important than in typical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for colname in df.columns[5:-1]:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    ax1.bar(list(range(0, len(df[df[\"label\"] == 1][colname].array))), df[df[\"label\"] == 1][colname].array)\n",
    "    ax1.set_xlabel(\"Speech Name\")\n",
    "    ax1.set_ylabel(f\"{colname} score\")\n",
    "    ax1.set_title(f\"Important Speeches vs {colname} Score\")\n",
    "    ax2.bar(list(range(0, len(df[df[\"label\"] == 0][colname].array))), df[df[\"label\"] == 0][colname].array)\n",
    "    ax2.set_xlabel(\"Speech Name\")\n",
    "    ax2.set_ylabel(f\"{colname} score\")\n",
    "    ax2.set_title(f\"Typical Speeches vs {colname} Score\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for colname in df.columns[5:-1]:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.violinplot(x=\"label\", y=colname, data=df)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### The influence of fear, neutrality and subjectivity, the proportion of named entities and stop‚Äê words, and the proportion of imagery words on speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "figure(figsize=(10, 10))\n",
    "plt.bar(list(range(0, len(df[df[\"label\"] == 1][\"fear\"].array))), df[df[\"label\"] == 1][\"fear\"].array, label=\"Important\",\n",
    "        color=\"#005e77\")\n",
    "plt.bar(list(range(0, len(df[df[\"label\"] == 0][\"fear\"].array))), df[df[\"label\"] == 0][\"fear\"].array, label=\"Typical\",\n",
    "        color=\"#8c949a\")\n",
    "plt.plot(list(range(0, len(df[df[\"label\"] == 0][\"fear\"].array))),\n",
    "         0.2 * np.ones(len(df[df[\"label\"] == 0][\"fear\"].array)), color=\"#2f4d5d\", linestyle=\"--\")\n",
    "plt.xlim(0, 77)\n",
    "plt.xlabel(\"Speech\")\n",
    "plt.ylabel(\"Fear\")\n",
    "plt.legend(loc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "figure(figsize=(10, 4))\n",
    "plt.plot(list(range(0, len(df[df[\"label\"] == 1][\"neutral\"].array))), df[df[\"label\"] == 1][\"neutral\"].array,\n",
    "         label=\"Neutrality\", color=\"#005e77\")\n",
    "plt.plot(list(range(0, len(df[df[\"label\"] == 1][\"subjectivity\"].array))), df[df[\"label\"] == 1][\"subjectivity\"].array,\n",
    "         label=\"Subjectivity\", color=\"#8c949a\")\n",
    "plt.plot(list(range(0, len(df[df[\"label\"] == 0][\"fear\"].array))),\n",
    "         0.2 * np.ones(len(df[df[\"label\"] == 0][\"neutral\"].array)), color=\"#005e77\", linestyle=\"--\")\n",
    "plt.plot(list(range(0, len(df[df[\"label\"] == 0][\"fear\"].array))),\n",
    "         0.4 * np.ones(len(df[df[\"label\"] == 0][\"subjectivity\"].array)), color=\"#8c949a\", linestyle=\"--\")\n",
    "\n",
    "plt.xlim(0, 77)\n",
    "plt.xlabel(\"Speech\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend(loc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "figure(figsize=(10, 4))\n",
    "plt.scatter(list(range(0, len(df[df[\"label\"] == 1][\"entities_proportion_in_speech\"].array))),\n",
    "            df[df[\"label\"] == 1][\"entities_proportion_in_speech\"].array, label=\"Proportion of entities\",\n",
    "            color=\"#005e77\")\n",
    "plt.scatter(list(range(0, len(df[df[\"label\"] == 1][\"stopwords_proportion_in_speech\"].array))),\n",
    "            df[df[\"label\"] == 1][\"stopwords_proportion_in_speech\"].array, label=\"Proportion of stopwords\",\n",
    "            color=\"#8c949a\")\n",
    "plt.plot(list(range(0, len(df[df[\"label\"] == 0][\"fear\"].array))),\n",
    "         0.02 * np.ones(len(df[df[\"label\"] == 0][\"entities_proportion_in_speech\"].array)), color=\"#005e77\",\n",
    "         linestyle=\"--\")\n",
    "plt.plot(list(range(0, len(df[df[\"label\"] == 0][\"fear\"].array))),\n",
    "         0.05 * np.ones(len(df[df[\"label\"] == 0][\"entities_proportion_in_speech\"].array)), color=\"#8c949a\",\n",
    "         linestyle=\"--\")\n",
    "plt.xlim(0, 77)\n",
    "plt.xlabel(\"Speech\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend(loc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "figure(figsize=(10, 4))\n",
    "plt.bar(list(range(0, len(df[df[\"label\"] == 1][\"imagery_proportion_in_speech\"].array))),\n",
    "        df[df[\"label\"] == 1][\"imagery_proportion_in_speech\"].array, label=\"Important\", color=\"#005e77\")\n",
    "plt.bar(list(range(0, len(df[df[\"label\"] == 0][\"imagery_proportion_in_speech\"].array))),\n",
    "        df[df[\"label\"] == 0][\"imagery_proportion_in_speech\"].array, label=\"Typical\", color=\"#8c949a\")\n",
    "plt.plot(list(range(0, len(df[df[\"label\"] == 0][\"fear\"].array))),\n",
    "         0.075 * np.ones(len(df[df[\"label\"] == 0][\"imagery_proportion_in_speech\"].array)), color=\"#2f4d5d\",\n",
    "         linestyle=\"--\")\n",
    "plt.xlim(0, 77)\n",
    "plt.xlabel(\"Speech\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend(loc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Correlation Analysis of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = df.iloc[:, 5:-1]\n",
    "correlation_d = data.corr()\n",
    "mask = np.triu(np.ones_like(correlation_d, dtype=bool))\n",
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "sns.heatmap(correlation_d, mask=mask, cmap=cmap, center=0,\n",
    "            square=True, linewidths=.5)\n",
    "\n",
    "corr_abs = correlation_d.abs()\n",
    "unstacked_v = corr_abs.unstack()\n",
    "sort_v = pd.DataFrame(unstacked_v.sort_values(kind='quicksort'), columns=['correlation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Classification\n",
    "### Logistic Regression, Support Vectors, Random Forest (Bagging approach), and Extreme Gradient (Boosting approach)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grid_param_dict = {}\n",
    "N_FEATURES_OPTIONS = [2, 4, 6, 8, 10, 12, 14, 15]\n",
    "C_OPTIONS = [1, 10, 100, 1000]\n",
    "k_neigbors = [1, 2, 3, 4, 5]\n",
    "\n",
    "param_grid_XGBoost = {\"XGBoost\": [{\n",
    "    'encoder': [StandardScaler(), RobustScaler()],\n",
    "    \"clf__learning_rate\": [0.05, 0.10, 0.15, 0.20, 0.25, 0.30],\n",
    "    \"clf__max_depth\": [3, 4, 5, 6, 8, 10, 12, 15],\n",
    "    \"clf__min_child_weight\": [1, 3, 5, 7],\n",
    "    \"clf__gamma\": [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "    \"clf__colsample_bytree\": [0.3, 0.4, 0.5, 0.7]}]}\n",
    "param_grid_SVC = {\"svm\": [{\n",
    "    'encoder': [StandardScaler(), RobustScaler()],\n",
    "    'clf__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'clf__gamma': [100, 10, 1, 0.1, 0.2, 0.02, 0.002, 0.0002, 0.01, 0.001, 0.0001],\n",
    "    'clf__kernel': ['rbf', 'linear']}]}\n",
    "param_grid_LR = {\"logistic\": [{\n",
    "    'encoder': [StandardScaler(), RobustScaler()],\n",
    "    'clf__penalty': ['l1', 'l2'],\n",
    "    'clf__C': [0.001, 0.01, 0.1, 1],\n",
    "    'clf__solver': ['liblinear']}]}\n",
    "param_grid_RF = {\"RandomForest\": [{\n",
    "    'encoder': [StandardScaler(), RobustScaler()],\n",
    "    'clf__min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'clf__max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'clf__min_samples_split': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}]}\n",
    "\n",
    "grid_param_dict.update(param_grid_SVC)\n",
    "grid_param_dict.update(param_grid_LR)\n",
    "grid_param_dict.update(param_grid_RF)\n",
    "grid_param_dict.update(param_grid_XGBoost)\n",
    "\n",
    "saving_dir = f\"{cwd}/results/MLA_compare.csv\"\n",
    "data = np.array(df.iloc[:, 5:-1])\n",
    "target = np.array(df.iloc[:, -1])\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3,\n",
    "                                                    stratify=target, random_state=109)\n",
    "model_dict = {\n",
    "    'logistic': LogisticRegression(random_state=11, max_iter=10000),\n",
    "    'svm': svm.SVC(probability=True),\n",
    "    'RandomForest': RandomForestClassifier(verbose=True, n_estimators=500),\n",
    "    'XGBoost': XGBClassifier(verbose=True),\n",
    "}\n",
    "\n",
    "classification_comparison_columns = []\n",
    "classification_comparison_columns = pd.DataFrame(columns=classification_comparison_columns)\n",
    "results = []\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "for i, model in enumerate(model_dict.keys()):\n",
    "    print(model)\n",
    "    start = datetime.now()\n",
    "    pipe = Pipeline(steps=[('encoder', \"passthrough\"), (\"reduce_dim\", \"passthrough\"), \n",
    "                           ('clf', model_dict[model])])\n",
    "\n",
    "    grid = GridSearchCV(pipe, n_jobs=-1, param_grid=grid_param_dict[model],\n",
    "                        cv=cv, scoring='accuracy', refit=True, verbose=0)\n",
    "    grid.fit(X_train, y_train)\n",
    "    end = datetime.now()\n",
    "\n",
    "    y_pred = grid.predict(X_test)\n",
    "    y_train_pred = grid.predict(X_train)\n",
    "    y_train_pred_scores = grid.predict_proba(X_train)[:, 1]\n",
    "    y_test_pred_scores = grid.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    roc_train = roc_auc_score(y_train, y_train_pred_scores)\n",
    "    accuracy_train = metrics.accuracy_score(y_train, y_train_pred)\n",
    "    roc_test = roc_auc_score(y_test, y_test_pred_scores)\n",
    "    ac_test = metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "    MLA_name = model_dict[model].__class__.__name__\n",
    "    classification_comparison_columns.loc[i, 'Name'] = MLA_name\n",
    "    classification_comparison_columns.loc[i, 'Tuned Hyperparameters'] = str(grid.best_params_)\n",
    "    classification_comparison_columns.loc[i, 'Train_Accuracy'] = accuracy_train\n",
    "    classification_comparison_columns.loc[i, 'Test_Accuracy'] = round(ac_test, 4)\n",
    "    classification_comparison_columns.loc[i, 'Tran_ROC'] = round(roc_train, 4)\n",
    "    classification_comparison_columns.loc[i, 'Test_ROC'] = round(roc_test, 4)\n",
    "    classification_comparison_columns.loc[i, 'duration_minutes'] = (end - start).total_seconds() / 60\n",
    "classification_comparison_columns.to_csv(saving_dir, sep=',', index=False)\n",
    "classification_comparison_columns['Tuned Hyperparameters']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Final Chosen Model and Feature Importance\n",
    "### Random Forests\n",
    "#### Test Accuracy: 76.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = np.array(df.iloc[:, 5:-1])\n",
    "target = np.array(df.iloc[:, -1])\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3,\n",
    "                                                    stratify=target, random_state=109)\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled=scaler.transform(X_test)\n",
    "\n",
    "model=RandomForestClassifier(verbose = 0, n_estimators=500, max_depth= 4,\n",
    "                             min_samples_leaf= 2,min_samples_split= 42)\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_train_pred = model.predict(X_train_scaled)\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "y_train_pred_scores=model.predict_proba(X_train_scaled)[:,1]\n",
    "y_test_pred_scores=model.predict_proba(X_test_scaled)[:,1]\n",
    "\n",
    "roc_train=roc_auc_score(y_train, y_train_pred_scores)\n",
    "roc_test=roc_auc_score(y_test, y_test_pred_scores)\n",
    "ac_train= metrics.accuracy_score(y_train, y_train_pred)\n",
    "ac_test= metrics.accuracy_score(y_test, y_test_pred)\n",
    "print(\"Train Accuracy :\",ac_train)\n",
    "print(\"Test  Accuracy :\",ac_test)\n",
    "print(\"Train ROC Accuracy :\",roc_train)\n",
    "print(\"Test ROC Accuracy :\",roc_test)\n",
    "\n",
    "\n",
    "feature_names =df.iloc[:, 5:-1].columns\n",
    "df_X_train_scaled = pd.DataFrame(X_train_scaled,columns=feature_names)\n",
    "df_X_test_scaled = pd.DataFrame(X_test_scaled,columns=feature_names)\n",
    "explainer = shap.Explainer(model.predict, df_X_test_scaled)\n",
    "shap_values = explainer(df_X_test_scaled)\n",
    "shap.summary_plot(shap_values, plot_type='violin')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mda-project",
   "language": "python",
   "name": "mda-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}